{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Markdown of K-Means.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnandKhandekar/GMM-with-EM-algortihm/blob/master/Markdown_of_K_Means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYiFxtlZjW2X",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "# **K-Means **\n",
        "***\n",
        "\n",
        "Ref : http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html#Assessing-Convergence\n",
        "\n",
        "Ref : https://krasserm.github.io/2019/11/21/latent-variable-models-part-1/\n",
        "\n",
        "Ref : https://github.com/Adaickalavan/Machine-Learning-CSMM102x-John-Paisley-Columbia-University-EdX/blob/master/Week%209%20Clustering/hw3_clustering.py\n",
        "\n",
        "Ref : Pattern Recognition and Machine Learning , Bishop. Springer Publication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRhn7yz3qBUI",
        "colab_type": "text"
      },
      "source": [
        "$\\newcommand{\\R}{\\mathbb{R}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcckFtf0zr65",
        "colab_type": "text"
      },
      "source": [
        "# 1. K Means Clustering\n",
        "\n",
        "> We begin by considering the problem of identifying groups, or clusters, of data points in a multidimensional space. Suppose we have a data set {x1, . . . , xN} consisting of N observations of a random D-dimensional Euclidean variable x. Our goal is to partition the data set into some number K of clusters, where we shall suppose for\n",
        "the moment that the value of K is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster. We can formalize this notion by\n",
        "first introducing a set of D-dimensional vectors $\\mu_k$ where k = 1, . . . , K, in which $\\mu_k$ is a prototype associated with the kth cluster. As we shall see shortly, we can think of the $\\mu_k$ as representing the centres of the clusters. Our goal is then to find\n",
        "an assignment of data points to clusters, as well as a set of vectors {μk}, such that the sum of the squares of the distances of each data point to its closest vector $\\mu_k$, is\n",
        "a minimum.\n",
        "\n",
        "> Let us define some notation to describe the assignment\n",
        "of data points to clusters. For each data point xn, we introduce a corresponding set of binary indicator variables $r_{n}{_k}$ ∈ {0, 1}, where k = 1, . . . , K describing which of the K clusters the data point $x_n$ is assigned to, so that if data point xn is assigned to\n",
        "cluster k then $r_{n}{_k}$ = 1, and $r_{n}{_k}$ = 0 for j \t= k. This is known as the 1-of-K coding\n",
        "scheme. We can then define an objective function, sometimes called a distortion measure, given by\n",
        " $J = \\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}||x_n-\\mu_k||^2 $\n",
        "\n",
        "> J represents the sum of the squares of the distances of each data point to it assigned vector $\\mu_k$. μk. Our goal is to find values for the $r_{n}{_k}$ and the $\\mu_k$ so as to\n",
        "minimize J. \n",
        "* We can do this through an iterative procedure in which each iteration\n",
        "involves two successive steps corresponding to successive optimizations with respect\n",
        "to the $r_{n}{_k}$ and the $\\mu_k$.\n",
        "* First we choose some initial values for the $\\mu_k$. Then in the first\n",
        "phase we minimize J with respect to the $r_{n}{_k}$, keeping the $\\mu_k$ fixed. \n",
        "* In the second\n",
        "phase we minimize J with respect to the $\\mu_k$ , keeping $r_{n}{_k}$ fixed. \n",
        "* This two-stage\n",
        "optimization is then repeated until convergence.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idoVRe3cAYcD",
        "colab_type": "text"
      },
      "source": [
        "> **We shall see that these two stages\n",
        "of updating $r{_n}{_k}$ and updating $\\mu_k$ correspond respectively to the E (expectation) and\n",
        " (maximization) steps of the EM algorithm, and to emphasize this we shall use the\n",
        "terms E step and M step in the context of the K-means algorithm.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQgNZVL4CkYA",
        "colab_type": "text"
      },
      "source": [
        "# 2. Mixture of Gaussians\n",
        "\n",
        "> We now turn to a formulation of Gaussian mixtures in\n",
        "terms of discrete latent variables. This will provide us with a deeper insight into this\n",
        "important distribution, and will also serve to motivate the expectation-maximization\n",
        "algorithm.\n",
        "\n",
        "> The Gaussian mxture distributions can be wrtten as a linear superposition of Gaussians in the form :\n",
        "\n",
        "* $p(x) = \\sum_{k=1}^K \\pi_k N(x|\\mu_k,\\Sigma_k)$     ----eqn 1 \n",
        "\n",
        "> Let us introduce a K-dimensional binary random variable z having a 1-of-K representation\n",
        "in which a particular element $z_k$ is equal to 1 and all other elements are\n",
        "equal to 0. The values of $z_k$ therefore satisfy $z_k$ ∈ {0, 1} and $\\Sigma_kz_k=1$, and we\n",
        "see that there are K possible states for the vector z according to which element is\n",
        "nonzero.\n",
        "\n",
        "> We shall define the joint distribution p(x, z) in terms of a marginal distribution p(z) and a conditional distribution p(x|z), corresponding to the graphical model.\n",
        "\n",
        "> The marginal distribution over z is specified in terms of the\n",
        "mixing coefficients $\\pi_k$, such that : \n",
        "\n",
        "*  $p(z_k=1)=\\pi_k$  \n",
        "\n",
        ">  where the parameters {$\\pi_k$} must satisfy  $0\\leq\\pi_k\\leq1$  -----eqn2 together with $\\Sigma_{k=1}^K\\pi_k=1$  -----eqn3 in prder to be valid probabilities. \n",
        "> Because *z*  uses 1-of-K reepresentation, we can also write this distribution in the following form :\n",
        "*  $p_z= \\Sigma_{k=1}^K\\pi_k^z{^k}$   -- eqn 4 \n",
        "\n",
        "> Similarlly, the\n",
        " conditional distribution of x given aparticular value of z is a Gaussian \n",
        "*  $p(x,z)= \\prod_{k=1}^K \\mathcal{N}(x| \\mu_k, \\Sigma_k)$   -- eqn 5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX-oVnCUNbty",
        "colab_type": "text"
      },
      "source": [
        "> The joint distribution is given by p(z)p(x|z), and **the marginal distribution of x** is then obtained by **summing the joint distribution over all possible states of z** to give\n",
        "* $p(x) = \\Sigma_zp(z)P(x|z)= \\Sigma_{k=1}^K\\pi_k\\mathcal{N}(x|\\mu_k,\\Sigma_k)$    ---- eqn 6 \n",
        "\n",
        ">Thus the marginal of x is is a Gaussian mixture of the form  define above.\n",
        "> If we have several observations $x_1, . . . , x_N$,\n",
        "then, because we have represented the marginal distribution in the form $p(x) =\n",
        "\\Sigma_z p(x, z)$, it follows that for every observed data point $x_n$ there is a corresponding\n",
        "latent variable $z_n$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMaCr00vTKQE",
        "colab_type": "text"
      },
      "source": [
        "> We have therefore found an equivalent formulation of the Gaussian mixture involving\n",
        "an explicit latent variable. It might seem that we have not gained much\n",
        "by doing so. However, we are now able to work with the joint distribution $p(x, z)$ instead of the marginal distribution $p(x)$, and this will lead to significant simplifications,\n",
        "most notably through the introduction of the expectation-maximization (EM)\n",
        "algorithm.\n",
        "\n",
        "> Another quantity that will play an important role is the conditional probability\n",
        "of z given x. We shall use $γ(z_k)$ to denote $p(z_k = 1|x)$, whose value can be found\n",
        "using Bayes’ theorem\n",
        "\n",
        "* $γ(z_k) = p(z_k=1|x) = \\frac{p(z_k=1)p(x|z_k=1)}{\\Sigma_{j=1}^Kp(z_j=1)p(x|z_j=1)}$\n",
        "\n",
        "*  $γ(z_k) = p(z_k=1|x) = \\frac{\\pi_k\\mathcal{N}(x|\\mu_k,\\Sigma_k)}{\\Sigma_{j=1}^K\\pi_j\\mathcal{N}(x|\\mu_j,\\Sigma_j)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98dVNeMEa2b3",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "> We shall view **$π_k$ as the prior probability of $z_k = 1$, and the quantity $γ(z_k)$ as the\n",
        "corresponding posterior probability** once we have observed $x$. As we shall see later,\n",
        "**$γ(z_k)$ **can also be viewed as **the responsibility that component k takes for ‘explaining’\n",
        "the observation $x$.**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5UtnDQpc2gb",
        "colab_type": "text"
      },
      "source": [
        "# 3. Maximum Likehood\n",
        "\n",
        "> Suppose we have a data set of observations {$x_1, . . . , x_N$}, and we wish to model\n",
        "this data using a mixture of Gaussians. We can represent this data set as an N × D matrix X in which the nth row is given by $x_n^T$\n",
        ". Similarly, the corresponding latent\n",
        "variables will be denoted by an N × K matrix Z with rows $z_n^T$\n",
        ". If we assume that\n",
        "the data points are drawn independently from the distribution, then we can express\n",
        "the Gaussian mixture model for this i.i.d. data set using the graphical representation as shown inthe image below\n",
        "![alt text](https://drive.google.com/uc?id=1YXZ7Dl_QfS3cR4MZKurwOYR2thek6U2e)\n",
        "\n",
        ">The log Likelihood function from the eqn 1  is given by\n",
        "\n",
        "* $lnp(X|\\pi,\\mu,\\Sigma) = \\Sigma_{n=1}^N ln \\{{\\sum_{k=1}^K \\pi_k N(x|}\\mu_k,\\Sigma_k)\\}$     ---- eqn 8\n",
        "\n",
        ">Before discussing how to maximize this function, it is worth emphasizing that\n",
        "there is a significant problem associated with the maximum likelihood framework\n",
        "applied to Gaussian mixture models, due to the presence of singularities. For simplicity,\n",
        "consider a Gaussian mixture whose components have covariance matrices\n",
        "given by $Σ_k = σ^2_kI$, where $I$ is the unit matrix, although the conclusions will hold\n",
        "for general covariance matrices.\n",
        "\n",
        "> There are issues of singularity and severe Overfitting.\n",
        "We shall see that this difficulty does not occur if we adopt a Bayesian approach. For the moment,however, we simply note that in applying maximum likelihood to Gaussian mixture models we must take steps to avoid finding such pathological solutions and instead\n",
        "seek local maxima of the likelihood function that are well behaved. We can hope to avoid the singularities by using suitable heuristics, for instance by detecting when a Gaussian component is collapsing and resetting its mean to a randomly chosen value\n",
        "while also resetting its covariance to some large value, and then continuing with the\n",
        "optimization.\n",
        "\n",
        "> **A further issue in finding maximum likelihood solutions arises from the fact\n",
        "that for any given maximum likelihood solution, a K-component mixture will have\n",
        "a total of K! equivalent solutions corresponding to the K! ways of assigning K\n",
        "sets of parameters to K components.**\n",
        "\n",
        ">Maximizing the log likelihood function for a Gaussian mixture model\n",
        "turns out to be a more complex problem than for the case of a single Gaussian. The\n",
        "difficulty arises from the presence of the summation over k that appears inside the\n",
        "logarithm in eqn 8, so that the logarithm function no longer acts directly on the\n",
        "Gaussian. If we set the derivatives of the log likelihood to zero, we will no longer\n",
        "obtain a closed form solution.\n",
        "\n",
        "> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZUvPBH12wZF",
        "colab_type": "text"
      },
      "source": [
        "# 4. EM for Gaussian Mixtures\n",
        "\n",
        "> An elegant and powerful method for finding maximum likelihood solutions for\n",
        "models with latent variables is called the expectation-maximization algorithm, or EM\n",
        "algorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997)\n",
        "\n",
        "> * EM can be generalized to obtain the variational inference framework.\n",
        "\n",
        "> Let us begin by writing down the conditions that must be satisfied at a maximum\n",
        "of the likelihood function. Setting the derivatives of $lnp(X|\\pi,\\mu,\\Sigma)$ in (9.14) with\n",
        "respect to the means $μ_k$ of the Gaussian components to zero, we obtain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx9KaF3ikbUN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# EM for the GMM\n",
        "\n",
        "\n",
        "> **Algorithm** : Maximum Likelihood EM for GMM. \n",
        "\n",
        "> **Given** : $x_i, ......,x_n$ where $x \\in  \\R^d$\n",
        "\n",
        ">  **Goal**  :Maximize  the LOWER Bound $L$ =  $\\sum_{i=1}^{n}ln p (x_i | \\pi, \\mu,\\Sigma )$\n",
        "***\n",
        "> Iterate until $L$ is \"$small$\" \n",
        "\n",
        ">* **E-step** :  For $ i = 1,......,n$   set \n",
        "\n",
        ">* $ \\phi_i(k) = \\frac{\\pi_k N(x_i | \\mu_k,\\Sigma_k )}{\\sum_j\\pi_j N(x_i | \\mu_j,\\Sigma_j)}$ \n",
        "\n",
        ">Recall that **GMM's goal** is to output a set of soft assignments per data point (allocating the probability of that data point belonging to each one of the clusters). To begin with, let's just assume we actually know the parameters  **$\\pi_k$, $\\mu_k$  and $\\sum_k$**  **(from some random initialization)** and we need a formula to compute the soft assignments having fixed the values of all the other parameters.\n",
        "\n",
        "> The soft assignments are quantified by the responsibility vector  $\\phi$ . For each observation  i , we form a responsibility vector with elements  $\\phi_1$(1), $\\phi_1$(2) all the way up to  $\\phi_1$(K). Where  K  is the total number of clusters, or often referred to as the number of components. The cluster responsibilities for a single data point  i  should sum to 1.\n",
        "\n",
        ">In order to model more complex data distribution, we can use a linear combination of several Gaussians instead of using just one. To compute the mixture of Gaussians, we introduce a set of cluster weights,$\\pi_k$ , one for each cluster  $k$ where $\\sum_i^K\\pi_k = 1 $  and  $0≤\\pi_k≤1 $ (meaning that the sum must add up to one and each of them is between 0 and 1). This parameter tells us what's the prior probability that the data point in our data set  x  comes from the  kth  cluster. We can think it as controlling each cluster's size.\n",
        "\n",
        ">The next part of the equation,  $N(x_i | \\mu_k,\\Sigma_k) $  tells us the following :  Given that we knew that the observation comes from the $ k{^t}{^h}$  cluster, what is the likelihood of observing our data point  $x_i$  coming from this cluster. To compute this part, the scipy package provides a convenient function multivariate_normal.pdf that computes the likelihood of seeing a data point in a multivariate Gaussian distribution.\n",
        "\n",
        ">After multiplying the prior and the likelihood, we need to normalize over all possible cluster assignments so that the responsibility vector becomes a valid probability. And this is essentially the computation that's done for the E step.\n",
        "\n",
        "***\n",
        "\n",
        "> *   **M-step** :  For  $ k = 1,... ,K$, define $ n_k= \\sum{_i}{_=}{_1}{^n}\\phi_i(k)$\n",
        "\n",
        "and update the values as follows :\n",
        "\n",
        "  > *  $\\pi_k = \\frac{n_k}{n}, $  \n",
        "\n",
        ">   *  $ \\mu_k = \\frac{1}{n_k}\\sum{_i}{_=}{_1}{^n}\\phi_i(k)$       and\n",
        "\n",
        " >  * $ \\Sigma_k =   \\frac{1}{n_k} \\sum{_i}{_=}{_1}{^n}\\phi_i(k)(x_i-\\mu_k)(x_i-\\mu_k){^T}  $\n",
        "\n",
        " * NOTE :  The update value of $\\mu_k$ is used for updating $\\Sigma_k$\n",
        "\n",
        "> First, the cluster weights  $\\pi_k$ , show us how much each cluster is represented over all data points (each cluster's relative size). This weight is given by the ratio of the soft count $n_K$  over the total number of data points  N .\n",
        "\n",
        "> When updating our parameters' estimates for each cluster  k , we need to account for the associated weights $\\phi_i(k)$  for every one of our observation. So every time we're touching a data point  $x_i$  it's going to be multiplied by  $\\phi_i(k)$.\n",
        "\n",
        ">Another thing that's worth noticing is that, when we're updating the parameter $\\mu_k$  and $ \\Sigma_k$ , instead of dividing the summation with the raw count of the total number of data points in that cluster  N , we will use the effective number of observations in that cluster (the sum of the responsibilities in that cluster) as the denominator. This is denoted as  $ n_k= \\sum{_i}{_=}{_1}{^n}\\phi_i(k)$\n",
        "\n",
        "***\n",
        "\n",
        "## Assesing CONVERGENCE of the EM algorithm\n",
        "\n",
        "> Apart from training the model, we also want a way to monitor the convergence of the algorithm. We do so by computing the log likelihood of the data given the current estimates of our model parameters and responsibilities.\n",
        "\n",
        "> During the E-step we used the follwing formula to calculate the weighted probability of every data point $x_i$ coming form the cluster $j$ and summed up all the weighted values.\n",
        "\n",
        ">*  $ \\sum_j\\pi_j N(x_i | \\mu_j,\\Sigma_j) $  \n",
        "\n",
        "> If we were to assume the observed data points were generated independently, the likelihood of the data can be written as :\n",
        "> * $ p(x |\\pi,\\mu,\\Sigma) = \\prod{_n}{_=}{_1}{^N}\\sum{_j}{_=}{_1}{^K}\\pi_j N(x_i | \\mu_j, \\Sigma_j) $\n",
        "\n",
        "> This basically means that we multiply all the probability for every data point together to obtain a single number that estimates the likelihood of the data fitted under the model's parameter. We can take the log of this likelihood so that the product becomes a sum and it makes the computation a bit easier:\n",
        "\n",
        "> * $ ln(p(x |\\pi,\\mu,\\Sigma))= \\sum{_i}{_=}{_1}{^N}ln \\{ \\sum{_j}{_=}{_1}{^K}\\pi_j N(x_i | \\mu_j, \\Sigma_j\\}$\n",
        "\n",
        "> **If the log likelihood of the data occuring under the current model's parameter does not improve by a tolerance value that we've pre-specified, then the algorithm is deemed converged.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}